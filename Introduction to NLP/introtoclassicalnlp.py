# -*- coding: utf-8 -*-
"""IntroToClassicalNlp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16ocH8dtj6PdI2FHb1EJoW63_WlhIN6My

## Introduction to Classical Natural Language Processing ##

This notebook is a hands-on introduction to Classical NLP, that is, non-deep learning techniques of NLP.  It is designed to be used with the edX course on Natural Language Processing, from Microsoft.  

The topics covered align with the NLP tasks related to the various stages of the NLP pipeline: text processing, text exploration, building features, and application level tasks.

## 1. Introduction ##

** 1.1 NLTK Setup  **
   - NLTK is included with the Anaconda Distribution of Python, or can be downloaded directly from nltk.org. 
   - Once NLTK is installed, the text data files (corpora) should be downloaded.  See the following cell to start the download.
"""

import nltk

# uncomment the line below to download NLTK resources the first time NLTK is used and RUN this cell.
# when the "NLTK Downloader" dialog appears (takes 10-20 seconds), click on the "download" button 
#nltk.download()

"""** 1.2 Crash Course in Regular Expressions **

If you are new to using regular expressions, or would like a quick refresher, you can study the examples
and resulting output in the code cell below.

Here is a cheat sheet for the SEARCH BASICS (code examples follow below):

    Operator   Meaning       Example  Example meaning
    
    +          one or more   a+       look for 1 or more "a" characters 
    *          zero or more  a*       look for 0 or more "a" characters
    ?          optional      a?       look for 0 or 1 "a" characters
    []         choose 1      [abc]    look for "a" or "b" or "c"
    [-]        range         [a-z]    look for any character between "a" and "z"
    [^]        not           [^a]     look for character that is not "a"
    ()         grouping      (a-z)+   look for one of more occurences of chars between "a" and "z"
    (|)        or operator   (ey|ax)  look for strings "ey" or "ax"
    
    ab         follow        ab       look for character "a" followed by character "b"
    ^          start         ^a       look for character "a" at start of string/line
    $          end           a$       look for character "a" at end of string/line
    \s         whitespace    \sa      look for whitespace character followed by "a"
    .          any character a.b      look for "a" followed by any char followed by "b"
    
Common Uses:
   - re.search     finds first matching object
   - re.findall    returns all matching objects
   - re.sub        replaces matches with replacement string
"""

import re

# search for single char
re.search(r"x", "this is an extra helping")

# search for single char
re.search(r"x", "this is an extra helping").group(0)     # gives easier-to-read output

# find all occurences of any character between "a" and "z"
re.findall(r"[a-z]", "$34.33 cash.")

# find all occurences of either "name:" or "phone:"
re.findall(r"(name|phone):", "My name: Joe, my phone: (312)555-1212")

# find "lion", "lions" or "Lion", or "Lions"
re.findall(r"([Ll]ion)s?", "Give it to the Lions or the lion.")

# replace allll lowercase letters with "x"
re.sub("[a-z]", "x", "Hey.  I know this regex stuff...")

"""## 2. Text Processing ##

This section introduces some of the tasks and techniques used to acquire, clean, and normalize the text data.

** 2.1 Data Acquisition **

Issues:
   - how do I find the data I need?
   - is it already in digital form, or will it need OCR?
   - how much will it cost?  
   - will it be updated/expanded over time?  More costs?
   - (if CUSTOMER DATA), do I have the legal / privacy rights needed to use the data in the way I need for my application? 
   - do I have the safeguards needed to securely store the data?
"""

# import nltk
# nltk.download('gutenberg')
# shows how to access one of the gutenberg books included in NLTK
print("gutenberg book ids=", nltk.corpus.gutenberg.fileids())

# load words from "Alice in Wonderland"

alice = nltk.corpus.gutenberg.words("carroll-alice.txt")
print("len(alice)=", len(alice))
print(alice[:100])

# load words from "Monty Python and the Holy Grail"
# nltk.download('webtext')
grail = nltk.corpus.webtext.words("grail.txt")
print("len(grail)=", len(grail))

print(grail[:100])

"""** 2.2 Plain Text Extraction **

If your text data lives in a non-plain text file (WORD, POWERPOINT, PDF, HTML, etc.), you will need to use a “filter” to extract the plain text from the file.

Python has a number of libraries to extract plain text from popular file formats, but they are take searching and supporting code to use.

** 2.3 Word and Sentence Segmentation (Tokenization) **

Word Segmentation Issues:
   - Some languages don’t white space characters
   - Words with hyphens or apostrophes (Who’s at the drive-in?)
   - Numbers, currency, percentages, dates, times (04/01/2018, $55,000.00)
   - Ellipses, special characters
   
Sentence Segmentation Issues:
   - Quoted speech within a sentence
   - Abbreviations with periods (The Ph.D. was D.O.A) 

Tokenization Techniques  
   - Perl script (50 lines) with RegEx         (Grefenstette, 1999)  
   - maxmatch Algorithm: 

         themanranafterit  ->    the man ran after it  
         thetabledownthere ->    theta bled own there      (Palmer, 2000)
"""

# code example: simple version of maxmatch algorithm for tokenization (word segmentation)
def tokenize(str, dict):
    s = 0
    words = []
    
    while (s < len(str)):
        found = False
        
        # find biggest word in dict that matches str[s:xxx]
        for word in dict:
            lw = len(word)
            if (str[s:s+lw] == word):
                words.append(word)
                s += lw
                found = True
                break
        if (not found):
            words.append(str[s])
            s += 1

    print(words)
    #return words

# small dictionary of known words, longest words first
dict = ["before", "table", "theta", "after", "where", "there", "bled", "said", "lead", "man", "her", "own", "the", "ran", "it"]

# this algorithm is designed to work with languages that don't have whitespace characters
# so simulate that in our test
tokenize("themanranafterit", dict)      # works!
tokenize("thetabledownthere", dict)     # fails! It should be "the table down there"

# NLTK example: WORD segmentation
# nltk.download('punkt')
nltk.word_tokenize("the man, he ran after it's $3.23 dog on 03/23/2016.")

# NLTK example: SENTENCE segmentation

nltk.sent_tokenize('The man ran after it.  The table down there?  Yes, down there!')

"""** 2.4 Stopword Removal **

Stopwords are common words that are "not interesting" for the app/task at hand.  

Easy part – removing words that appear in list.  
Tricky part – what to use for stop words?  App-dependent.  Standard lists, high-frequency words in your text, …
"""

# code example: simple algorithm for removing stopwords
stoppers = "a is of the this code.".split() # see the differenc where full stop is included and not included
# stoppers = "a is of the this code".split()
# stoppers = "a is of the this".split()

def removeStopWords(stopWords, txt):
    newtxt = ' '.join([word for word in txt.split() if word not in stopWords])
    return newtxt

removeStopWords(stoppers, "this is a test of the stop word removal code.")

# NLTK example: removing stopwords
# nltk.download('stopwords')
from nltk.corpus import stopwords
# stops = stopwords.words("English")
stops = stopwords.words("english") # note that here I have changed the "English" to all lower case

print("len(stops)=", len(stops))

# removeStopWords(stops, "this is a test of the stop word removal code.")
removeStopWords(stops, "note that here I have changed the English to all lower case")
removeStopWords(stops, "Case removal is part of a larger task called Text Normalization, which includes:")

"""** 2.5 Case Removal **

Case removal is part of a larger task called *Text Normalization*, which includes:
   - case removal
   - stemming   (covered in next section)
   
Goal of Case removal – converting all text to, for example, lower case
"""

# code example: case removal
str = 'The man ran after it.  The table down there?  Yes, down there!'
str.lower()

"""** 2.6 Stemming **
   
Goal of Stemming:
    – stripping off endings and other pieces, called AFFIXES 
    – for English, this is prefixes and suffixes. 
    - convert word to its base word, called the LEMMA / STEM (e.g., foxes -> fox)
    
Porter Stemmer
  - 100+ cascading “rewrite” rules  
    ational  -> ate    	(e.g., relational -> relate)  
    ing	     -> <null>	(e.g., playing -> play)  
    sess     -> ss  	(e.g., grasses -> grass)
"""

# NLTK example: stemming

def stem_with_porter(words):
    porter = nltk.PorterStemmer()
    new_words = [porter.stem(w) for w in words]
    return new_words
    
def stem_with_lancaster(words):
    porter = nltk.LancasterStemmer()
    new_words = [porter.stem(w) for w in words]
    return new_words    
    
str = "Please don't unbuckle your seat-belt while I am driving, he said"

print("porter:", stem_with_porter(str.split()))
print()
print("lancaster:", stem_with_lancaster(str.split()))

"""## 3. Text Exploration ##

** 3.1 Frequency Analysis **

  - Frequency Analysis
  - Letter
  - Word
  - Bigrams
  - Plots
"""

# NLTK example: frequence analysis
import nltk
from nltk.corpus import gutenberg
from nltk.probability import FreqDist

# get raw text from "Sense and Sensibility" by Jane Austen
raw = gutenberg.raw("austen-sense.txt")
fd_letters = FreqDist(raw)

words = gutenberg.words("austen-sense.txt")
fd_words = FreqDist(words)
sas = nltk.Text(words)

# these 2 lines let us size the freq dist plot
import matplotlib.pyplot as plt
plt.figure(figsize=(20, 5)) 

# frequency plot for letters from SAS
fd_letters.plot(100)

# plt.figure(figsize=(20, 5)) 
# fd_words.plot(100)

# plt.figure(figsize=(20, 5)) 
# sas.plot(100)

# these 2 lines let us size the freq dist plot
import matplotlib.pyplot as plt
plt.figure(figsize=(20, 5)) 

# frequency plot for words from SAS
fd_words.plot(50)

"""** 3.2 Collocations **

These are interesting word pairs, usually formed by the most common bigrams.  Bigrams are collections of word pairs that occur together in the text.
"""

# let's look at collocations for our "Sense and Sensibility" text
sas.collocations()

"""Nice!  

Now we are getting a feel for the language and subjects of the text.

** 3.3 Long words **

Sometimes looking at the long words in a text can be revealing.  Let's try it on sas.
"""

# let's look at long words in the text
longWords = [w for w in set(words) if len(w) > 14]
longWords[:15]

"""** 3.3 Concordance Views **

Concordance views, also called Keywords in Context (KWIC), show the specifed word with the words that surround it in text.  These views can be helpful in understaning how the words are being used in the text.
"""

# Let's try looking at some of these recent words in a Concordance view
sas.concordance("affectionately")
print()

sas.concordance("correspondence")
print()

sas.concordance("dare")
print()

"""** 3.4 Other Exploration Task/Views **"""

# look at words similiar to a word
sas.similar("affection")

# these 2 lines let us size the freq dist plot
import matplotlib.pyplot as plt
plt.figure(figsize=(15, 4)) 

# look at words as they appear over time in the book/document
sas.dispersion_plot(["sense", "love", "heart", "listen", "man", "woman"])

"""## 4. Building Features ##

** 4.1 Bag-of-Words (BOW) **

One of the simplest features when dealing with multiple texts (like multiple documents, or multiple sentences within a document), is called Bag-of-Words.  It builds a vocabular from each word in the set of texts, and then a feature for each word, indicate the presence/absence of that word within each text. Sometimes, the count of the word is used in place of a presence flag.

A common way to represent a set of features like this is called a One-Hot vector.  For example, lets say our vocabular from our set of texts is:

    today, here, I, a, fine, sun, moon, bird, saw

The sentence we want to build a BOW for is:

    I saw a bird today.  
    
Using a 1/0 for each word in the vocabulary, our BOW encoded as a one-hot vector would be:

    1 0 1 1 0 0 1 1

** 4.2 N-Grams **

N-grams represent the sequence of N words that are found in a text.  They are commonly used as a model of the text language since they represent the frequence of words/phrases appearing in the text.  

Common types of N-grams:

unigrams   - these are the set of single words appearing in the text  
bigrams    - these are the set of word pairs, like "good day" or "big deal", from the text  
trigrams   - these are the set of word triples, like "really good food", from the text

To build bigrams for a text, you need to extract all possible word pairs from the text and count how many times each pair occurs.  Then, you can use the top N words (1000), or the top percent (50%) as your language model.

** 4.3 Morphological Parsing **

**Goal**: convert input word into its morphological parts.  For example:  “geese” would return goose + N + PL  


Morphological Parsing:
geese  ->  goose + N + PL  
caught ->  catch + V + PastPart  

Morphological parsing is related to stemming, but instead of mapping the word variants to a stem word, it labels  
the stem word and its affixes.

Morphological parsing, even for English, is quite involved

** 4.4 TD/IDF **

TD/IDF stands for Term Document Inverse Document Frequency.  "Term" here can be thought of as a word.  This is a measure of the  relative importance of a word within a document, in the context of multiple documents.  

We start with the TD part - this is simply a normalized frequency of the word in the document:  

   - (word count in document) / (total words in document)  
   
The IDF is a weighting of the uniquess of the word across all of the documents.  Here is the complete formula of TD/IDF:  

   - td_idf(t,d) = wc(t,d)/wc(d) / dc(t)/dc()  

where:  
   - wc(t,d) = # of occurrences of term t in doc d  
   - wc(d) = # of words in doc d  
   - dc(t) = # of docs that contain at least 1 occurrence of term t  
   - dc() = # of docs in collection

** 4.5 Word Sense Disambiguation (WSD) **

Related to POS tagging, WSD is use to distingish between difference senses of a word. Each sense of the word
uses the same POS tag, but means something different.  For example:

   - she served the King
   - she served the ball  
    
   - he took his money to the bank
   - he took his canoe to the bank  
     
   - I play bass guitar
   - I fish for bass

** 4.6 Anaphora Resolution **  \

Examples:
   - Sam and Bill left with the toys.  They were later found.

Who does "they" refer to in the above sentence?  Sam and Bill, or the toys?

** 4.7 Part-of-speech (POS) Tagging **

   - Verb, noun, adjective, etc.
   - Simple tag set: 19 word classes
   
“They refuse to permit us to obtain a refuse permit”

What tagset to use?
  - Brown Corpus (87 tags)
  - C5 tagset (61 tags)
  - Penn Treebank (45 tags)
  
Types of Taggers
  - Rule-based (e.g., regular expression)
  - Lookup (Unigram)
  - N-Gram
  - Hybrid and Backoff
  - Brill Tagger (learns rules)
  - HMM Tagger

** HMM Tagger **

Previosly, we introduce Hidden Markov Models with a weather example.  Here, we will show an example of how we can use a HMM to create a POS tagger.
"""

# before building the HMM Tagger, let's warm up with implementing our HMM weather example here

# states
start = -1; cold = 0; normal = 1; hot = 2; stateCount = 3
stateNames = ["cold", "normal", "hot"]

# outputs
hotChoc = 0; soda=1; iceCream = 2

timeSteps = 7

# state transition probabilities
trans = {}
trans[(start, cold)] = .1
trans[(start, normal)] = .8
trans[(start, hot)] = .1

trans[(cold, cold)] = .7
trans[(cold, normal)] = .1
trans[(cold, hot)] = .2

trans[(normal, cold)] = .3
trans[(normal, normal)] = .4
trans[(normal, hot)] = .3

trans[(hot, cold)] = .2
trans[(hot, normal)] = .4
trans[(hot, hot)] = .4

# state outputs
output = {}
output[(cold, hotChoc)] = .7
output[(cold, soda)] = .3
output[(cold, iceCream)] = 0

output[(normal, hotChoc)] = .1
output[(normal, soda)] = .7
output[(normal, iceCream)] = .2

output[(hot, hotChoc)] = 0
output[(hot, soda)] = .6
output[(hot, iceCream)] = .4

diary = [soda, soda, hotChoc, iceCream, soda, soda, iceCream]

# manage cell values and back pointers
cells = {}
backStates = {}

def computeMaxPrev(t, sNext):
    maxValue = 0
    maxState = 0
    
    for s in range(stateCount):
        value = cells[t, s] * trans[(s, sNext)]
        if (s == 0 or value > maxValue):
            maxValue = value
            maxState = s
            
    return (maxValue, maxState)
    
def viterbi(trans, output, diary):

    # special handling for t=0 which have no prior states)
    for s in range(stateCount):
        cells[(0, s)] = trans[(start, s)] * output[(s, diary[0])]
        
    # handle rest of time steps
    for t in range(1, timeSteps):
        for s in range(stateCount):
            maxValue, maxState = computeMaxPrev(t-1, s)
            backStates[(t,s)] = maxState
            cells[(t, s)] = maxValue * output[(s, diary[t])]
            #print("t=", t, "s=", s, "maxValue=", maxValue, "maxState=", maxState, "output=", output[(s, diary[t])], "equals=", cells[(t, s)])
        
    # walk thru cells backwards to get most probable path
    path = []
    
    for tt in range(timeSteps):
        t = timeSteps - tt - 1    # step t backwards over timesteps
        maxValue = 0
        maxState = 0
        
        for s in range(stateCount):
            value = cells[t, s] 
            if (s == 0 or value > maxValue):
                maxValue = value
                maxState = s
                
        path.insert(0, maxState)
        
    return path

# test our algorithm on the weather problem
path = viterbi(trans, output, diary)

print("Weather by days:")
for i in range(timeSteps):
    state = path[i]
    print("  day=", i+1, stateNames[state])

"""** HMM Tagger Overview**

We are going to use a Hidden Markov Model to help us assign Part-of-Speech tags (like noun, verb, adjective, etc.) to words in a sentence.  We treat the human author of the sentence as moving between different meaning states (POS tags) as they compose the sentence.  Those state are hidden from us, but we observe the words of the sentence (the output of the meaning states).

In our example here, we will use 4 POS tags from the 87 tag Brown corpus:

   - VB   (verb, base form)
   - TO   (infinitive marker)
   - NN   (common singular noun)
   - PPSS (other nominative pronoun)

We are given the state-to-state transition probabilities and the state-output probabilities (see next code cell).  We are also given the sentence to decode: "I WANT TO RACE".
"""

# OK, here is our HMM POS Tagger for this example

# states
start = -1; VB = 0; TO = 1; NN = 2; PPSS = 3; stateCount = 4
stateNames = ["VB", "TO", "NN", "PPSS"]

# outputs
I = 0; WANT = 1; To = 2; RACE=3

timeSteps = 4

# state transition probabilities
trans = {}
trans[(start, VB)] = .19
trans[(start, TO)] = .0043
trans[(start, NN)] = .041
trans[(start, PPSS)] = .067

trans[(VB, VB)] = .0038
trans[(VB, TO)] = .035
trans[(VB, NN)] = .047
trans[(VB, PPSS)] = .0070

trans[(TO, VB)] = .83
trans[(TO, TO)] = 0
trans[(TO, NN)] = .00047
trans[(TO, PPSS)] = 0

trans[(NN, VB)] = .0040
trans[(NN, TO)] = .016
trans[(NN, NN)] = .087
trans[(NN, PPSS)] = .0045

trans[(PPSS, VB)] = .23
trans[(PPSS, TO)] = .00079
trans[(PPSS, NN)] = .0012
trans[(PPSS, PPSS)] = .00014

# state outputs
output = {}
output[(VB, I)] = 0
output[(VB, WANT)] = .0093
output[(VB, To)] = 0
output[(VB, RACE)] = .00012

output[(TO, I)] = 0
output[(TO, WANT)] = 0
output[(TO, To)] = .99
output[(TO, RACE)] = 0

output[(NN, I)] = 0
output[(NN, WANT)] = .000054
output[(NN, To)] = 0
output[(NN, RACE)] = .00057

output[(PPSS, I)] = .37
output[(PPSS, WANT)] = 0
output[(PPSS, To)] = 0
output[(PPSS, RACE)] = 0


sentence = [I, WANT, To, RACE]
words = ["I", "WANT", "TO", "RACE"]

# manage cell values and back pointers
cells = {}
backStates = {}

def computeMaxPrev(t, sNext):
    maxValue = 0
    maxState = 0
    
    for s in range(stateCount):
        value = cells[t, s] * trans[(s, sNext)]
        if (s == 0 or value > maxValue):
            maxValue = value
            maxState = s
            
    return (maxValue, maxState)
    
def viterbi(trans, output, sentence):

    # special handling for t=0 which have no prior states)
    for s in range(stateCount):
        cells[(0, s)] = trans[(start, s)] * output[(s, sentence[0])]
        
    # handle rest of time steps
    for t in range(1, timeSteps):
        for s in range(stateCount):
            maxValue, maxState = computeMaxPrev(t-1, s)
            backStates[(t,s)] = maxState
            cells[(t, s)] = maxValue * output[(s, sentence[t])]
            #print("t=", t, "s=", s, "maxValue=", maxValue, "maxState=", maxState, "output=", output[(s, sentence[t])], "equals=", cells[(t, s)])
        
    # walk thru cells backwards to get most probable path
    path = []
    
    for tt in range(timeSteps):
        t = timeSteps - tt - 1    # step t backwards over timesteps
        maxValue = 0
        maxState = 0
        
        for s in range(stateCount):
            value = cells[t, s] 
            if (s == 0 or value > maxValue):
                maxValue = value
                maxState = s
                
        path.insert(0, maxState)
        
    return path

# test our algorithm on the POS TAG data
path = viterbi(trans, output, sentence)

print("Tagged Sentence:")
for i in range(timeSteps):
    state = path[i]
    print("  word=", words[i], "\ttag=", stateNames[state])

# Here is an example of using the NLTK POS tagger
import nltk
# nltk.download('averaged_perceptron_tagger')
nltk.pos_tag("they refuse to permit us to obtain the refuse permit".split())

# POS tagging with supervised learning, using word suffix parts as features
import nltk
# nltk.download('brown')

# start by finding the most common 1, 2, and 3 character suffixes of words (using Brown corpus of 1.1 million words)
from nltk.corpus import brown
fd = nltk.FreqDist()        # create an empty one that we will count with

for word in brown.words():
    wl = word.lower()
    fd[wl[-1:]] += 1
    fd[wl[-2:]] += 1
    fd[wl[-3:]] += 1

topSuffixes = [ key for (key,value) in fd.most_common(30)]
print(topSuffixes[:40])

def pos_features(word):
    features = {}
    
    for suffix in topSuffixes:
        features[suffix] = word.lower().endswith(suffix)
        
    return features

#pos_features("table")

tagWords = brown.tagged_words(categories="news")
data = [(pos_features(word), tag) for (word,tag) in tagWords]

print("len(data)=", len(data))

dataCount = len(data)
trainCount = int(.8*dataCount)

trainData = data[:trainCount]
testData = data[trainCount:]

dtree = nltk.DecisionTreeClassifier.train(trainData)
# dtree = nltk.NaiveBayesClassifier.train(trainData) # don't forget to comment the 48th line as Naive_Bayes has no objct "pseudocode"

print("train accuracy=", nltk.classify.accuracy(dtree, trainData))
print("test accuracy=", nltk.classify.accuracy(dtree, testData))

print(dtree.classify(pos_features("cats")))
print(dtree.classify(pos_features("house")))

print(dtree.pseudocode(depth=4))

"""## 5. Classical NLP Applications ##

** 5.1 Name Gender Classifier **
"""

# code to build a classifier to classify names as male or female
# demonstrates the basics of feature extraction and model building
# nltk.download('names')
names = [(name, 'male') for name in nltk.corpus.names.words("male.txt")]
names += [(name, 'female') for name in nltk.corpus.names.words("female.txt")]

def extract_gender_features(name):
    name = name.lower()
    features = {}
    features["suffix"] = name[-1:]
    features["suffix2"] = name[-2:] if len(name) > 1 else name[0]
    features["suffix3"] = name[-3:] if len(name) > 2 else name[0]
    #features["suffix4"] = name[-4:] if len(name) > 3 else name[0]
    #features["suffix5"] = name[-5:] if len(name) > 4 else name[0]
    #features["suffix6"] = name[-6:] if len(name) > 5 else name[0]
    features["prefix"] = name[:1]
    features["prefix2"] = name[:2] if len(name) > 1 else name[0]
    features["prefix3"] = name[:3] if len(name) > 2 else name[0]
    features["prefix4"] = name[:4] if len(name) > 3 else name[0]
    features["prefix5"] = name[:5] if len(name) > 4 else name[0]
    #features["wordLen"] = len(name)
    
    #for letter in "abcdefghijklmnopqrstuvwyxz":
    #    features[letter + "-count"] = name.count(letter)
   
    return features

data = [(extract_gender_features(name), gender) for (name,gender) in names]

import random
random.shuffle(data)

#print(data[:10])
#print()
#print(data[-10:])

dataCount = len(data)
trainCount = int(.8*dataCount)

trainData = data[:trainCount]
testData = data[trainCount:]
bayes = nltk.NaiveBayesClassifier.train(trainData)

def classify(name):
    label = bayes.classify(extract_gender_features(name))
    print("name=", name, "classifed as=", label)

print("trainData accuracy=", nltk.classify.accuracy(bayes, trainData))
print("testData accuracy=", nltk.classify.accuracy(bayes, testData))

bayes.show_most_informative_features(25)

# print gender classifier errors so we can design new features to identify the cases
errors = []

for (name,label) in names:
    if bayes.classify(extract_gender_features(name)) != label:
        errors.append({"name": name, "label": label})

errors

"""** 5.2 Sentiment Analysis **"""

# movie reviews / sentiment analysis - part #1
from nltk.corpus import movie_reviews as reviews
import random
nltk.download('movie_reviews')
docs = [(list(reviews.words(id)), cat)  for cat in reviews.categories() for id in reviews.fileids(cat)]
random.shuffle(docs)

#print([ (len(d[0]), d[0][:2], d[1]) for d in docs[:10]])

fd = nltk.FreqDist(word.lower() for word in reviews.words())
topKeys = [ key for (key,value) in fd.most_common(2000)]

# movie reviews sentiment analysis - part #2
import nltk


def review_features(doc):
    docSet = set(doc)
    features = {}
    
    for word in topKeys:
        features[word] = (word in docSet)
        
    return features

#review_features(reviews.words("pos/cv957_8737.txt"))

data = [(review_features(doc), label) for (doc,label) in docs]

dataCount = len(data)
trainCount = int(.8*dataCount)

trainData = data[:trainCount]
testData = data[trainCount:]
bayes2 = nltk.NaiveBayesClassifier.train(trainData)

print("train accuracy=", nltk.classify.accuracy(bayes2, trainData))
print("test accuracy=", nltk.classify.accuracy(bayes2, testData))

bayes2.show_most_informative_features(20)

"""** 5.3 Named Entity Recognition (NER) **

Popular Named Entity Types:
  - ORGANIZATION
  - PERSON
  - LOCATION
  - GPE
  - DATE
  
Named Entity Extraction Techniques:
  - Chunking (tag pattern to group)
  - Chinking (tag pattern to omit)
  - Nested chunks (recursion)
  - Hand-crafted rules
  - Rules learned from data
"""

# Named Entity Regcognition (NER) 
# processes sentences and produces (entity, relation, entity) triples!
import nltk

# first, process the document by separating the text into sentences, then words within sentences, then tag words by sentence
def preprocess(doc):
    sents = nltk.sent_tokenize(doc) 
    sents2 = [nltk.word_tokenize(sent) for sent in sents]
    sents3 = [nltk.pos_tag(sent) for sent in sents2]
    
# we are going to use a technique called CHUNKING where we label sequences of POS tags as a high level tag, like a 
# noun phrase (NP).  

# here we test our idea with a simple sentence, and a grammar for detectecting NP.  The grammar:
#  <NP>  ::= [ <DT> ] [ <JJ list> ] <NN>

tagged_sent = [("the", "DT"), ("little", "JJ"), ("yellow", "JJ"), ("dog", "NN"), ("barked", "VBD"), ("at", "IN"),
              ("the", "DT"), ("cat", "NN")]

#np_grammar = "NP: {<DT>?<JJ>*<NN>}"
#np_grammar = "NP: {<DT>?<JJ.*>*<NN.*>*}"

np_grammar = r""" 
    NP:  
      {<DT|PP\$>?<JJ>*<NN>}
      {<NPP>+}
      """

parser = nltk.RegexpParser(np_grammar)
result = parser.parse(tagged_sent)

print(result.__repr__())